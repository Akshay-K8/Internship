{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Task 1 : MOVIE GENRE CLASSIFICATION </h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aksha\\AppData\\Local\\Temp\\ipykernel_20116\\951752079.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df= pd.read_csv(path, sep=':::', names=['ID','Title', 'Genre', 'Description'])\n"
     ]
    }
   ],
   "source": [
    "path = \"./Genre Classification Dataset/train_data.txt\"\n",
    "df= pd.read_csv(path, sep=':::', names=['ID','Title', 'Genre', 'Description'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drama</td>\n",
       "      <td>Listening in to a conversation between his do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thriller</td>\n",
       "      <td>A brother and sister with a past incestuous r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adult</td>\n",
       "      <td>As the bus empties the students for their fie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>drama</td>\n",
       "      <td>To help their unemployed father make ends mee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>drama</td>\n",
       "      <td>The film's title refers not only to the un-re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54209</th>\n",
       "      <td>comedy</td>\n",
       "      <td>This short-lived NBC live sitcom centered on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54210</th>\n",
       "      <td>horror</td>\n",
       "      <td>The NEXT Generation of EXPLOITATION. The sist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54211</th>\n",
       "      <td>documentary</td>\n",
       "      <td>Ze bestaan echt, is a stand-up comedy about g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54212</th>\n",
       "      <td>comedy</td>\n",
       "      <td>Walter and Vivian live in the country and hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54213</th>\n",
       "      <td>history</td>\n",
       "      <td>On Labor Day Weekend, 1935, the most intense ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54214 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Genre                                        Description\n",
       "0             drama    Listening in to a conversation between his do...\n",
       "1          thriller    A brother and sister with a past incestuous r...\n",
       "2             adult    As the bus empties the students for their fie...\n",
       "3             drama    To help their unemployed father make ends mee...\n",
       "4             drama    The film's title refers not only to the un-re...\n",
       "...              ...                                                ...\n",
       "54209        comedy    This short-lived NBC live sitcom centered on ...\n",
       "54210        horror    The NEXT Generation of EXPLOITATION. The sist...\n",
       "54211   documentary    Ze bestaan echt, is a stand-up comedy about g...\n",
       "54212        comedy    Walter and Vivian live in the country and hav...\n",
       "54213       history    On Labor Day Weekend, 1935, the most intense ...\n",
       "\n",
       "[54214 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df[['Genre','Description']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['next',\n",
       " 'generation',\n",
       " 'exploitation',\n",
       " 'sisters',\n",
       " 'kapa',\n",
       " 'bay',\n",
       " 'sorority',\n",
       " 'house',\n",
       " 'mysteriously',\n",
       " 'vanish',\n",
       " 'girls',\n",
       " 'abduct',\n",
       " 'lock',\n",
       " 'horse',\n",
       " 'stable',\n",
       " 'force',\n",
       " 'sexual',\n",
       " 'slavery',\n",
       " 'depravity',\n",
       " 'lead',\n",
       " 'fight',\n",
       " 'final',\n",
       " 'breath',\n",
       " 'electric',\n",
       " 'ear',\n",
       " 'devices',\n",
       " 'plant',\n",
       " 'prevent',\n",
       " 'escape',\n",
       " 'electric',\n",
       " 'shock',\n",
       " 'therapy',\n",
       " 'diabolical',\n",
       " 'family',\n",
       " 'incest',\n",
       " 'ride',\n",
       " 'tribe',\n",
       " 'obese',\n",
       " 'woman',\n",
       " 'never',\n",
       " 'stop',\n",
       " 'eat',\n",
       " 'midget',\n",
       " 'voice',\n",
       " 'box',\n",
       " 'parent',\n",
       " 'two',\n",
       " 'sons',\n",
       " 'repulsive',\n",
       " 'drool',\n",
       " 'inbreds',\n",
       " 'speech',\n",
       " 'problems',\n",
       " 'scabbed',\n",
       " 'head',\n",
       " 'daughter',\n",
       " 'beautiful',\n",
       " 'young',\n",
       " 'woman',\n",
       " 'cover',\n",
       " 'scar',\n",
       " 'mentally',\n",
       " 'physically',\n",
       " 'dead',\n",
       " 'girls',\n",
       " 'cry',\n",
       " 'exploitation',\n",
       " 'revenge',\n",
       " 'film',\n",
       " 'combine',\n",
       " 'much',\n",
       " 'one',\n",
       " 'push',\n",
       " 'boundaries',\n",
       " 'today',\n",
       " 'horror']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Description'][54210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " drama           13613\n",
       " documentary     13096\n",
       " comedy           7447\n",
       " short            5073\n",
       " horror           2204\n",
       " thriller         1591\n",
       " action           1315\n",
       " western          1032\n",
       " reality-tv        884\n",
       " family            784\n",
       " adventure         775\n",
       " music             731\n",
       " romance           672\n",
       " sci-fi            647\n",
       " adult             590\n",
       " crime             505\n",
       " animation         498\n",
       " sport             432\n",
       " talk-show         391\n",
       " fantasy           323\n",
       " mystery           319\n",
       " musical           277\n",
       " biography         265\n",
       " history           243\n",
       " game-show         194\n",
       " news              181\n",
       " war               132\n",
       "Name: Genre, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Genre'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Text Preprocessing\n",
    "<h4> basic preprocessing and remove punctutaions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def preprocess(data):\n",
    "    data=str(data).lower().strip()\n",
    "\n",
    "    data=data.replace('%',' percent ') \n",
    "    data=data.replace('$',' doller ') \n",
    "    data=data.replace('@',' at ') \n",
    "    data=data.replace('-',' ') \n",
    "\n",
    "    contractions = {\n",
    "    \"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "    \"aren't\": \"are not / am not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he had / he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he shall / he will\",\n",
    "    \"he'll've\": \"he shall have / he will have\",\n",
    "    \"he's\": \"he has / he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how has / how is / how does\",\n",
    "    \"I'd\": \"I had / I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I shall / I will\",\n",
    "    \"I'll've\": \"I shall have / I will have\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it had / it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it shall / it will\",\n",
    "    \"it'll've\": \"it shall have / it will have\",\n",
    "    \"it's\": \"it has / it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she had / she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she shall / she will\",\n",
    "    \"she'll've\": \"she shall have / she will have\",\n",
    "    \"she's\": \"she has / she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as / so is\",\n",
    "    \"that'd\": \"that would / that had\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that has / that is\",\n",
    "    \"there'd\": \"there had / there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there has / there is\",\n",
    "    \"they'd\": \"they had / they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they shall / they will\",\n",
    "    \"they'll've\": \"they shall have / they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we had / we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what shall / what will\",\n",
    "    \"what'll've\": \"what shall have / what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what has / what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when has / when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where has / where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who shall / who will\",\n",
    "    \"who'll've\": \"who shall have / who will have\",\n",
    "    \"who's\": \"who has / who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why has / why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you had / you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you shall / you will\",\n",
    "    \"you'll've\": \"you shall have / you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "    }\n",
    "\n",
    "    data_decontracted=[]\n",
    "\n",
    "    for word in data.split():\n",
    "        if word in contractions:\n",
    "            word = contractions[word]\n",
    "\n",
    "        data_decontracted.append(word)\n",
    "\n",
    "    data = ' '.join(data_decontracted)\n",
    "    data = data.replace(\"'ve\",\" have \")\n",
    "    data = data.replace(\"n't\",\" not \")\n",
    "    data = data.replace(\"'re\",\" are \")\n",
    "    data = data.replace(\"'ll\",\" will \")\n",
    "\n",
    "    pattern = re.compile('\\W')\n",
    "    data = re.sub(pattern, ' ', data).strip()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "df['Description']=df['Description'].apply(lambda x : preprocess(x))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Removing Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['Description']=df['Description'].apply(lambda x : remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> word tokenize and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatization(desclist):\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word,pos='v') for word in desclist]\n",
    "    return lemmatized_words\n",
    "\n",
    "df['Description']=df['Description'].apply(lambda x: word_tokenize(x))\n",
    "df['Description']=df['Description'].apply(lambda x : lemmatization(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming 'y' is your target column\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['Genre'])\n",
    "\n",
    "# Get the label mapping\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' action ': 0,\n",
       " ' adult ': 1,\n",
       " ' adventure ': 2,\n",
       " ' animation ': 3,\n",
       " ' biography ': 4,\n",
       " ' comedy ': 5,\n",
       " ' crime ': 6,\n",
       " ' documentary ': 7,\n",
       " ' drama ': 8,\n",
       " ' family ': 9,\n",
       " ' fantasy ': 10,\n",
       " ' game-show ': 11,\n",
       " ' history ': 12,\n",
       " ' horror ': 13,\n",
       " ' music ': 14,\n",
       " ' musical ': 15,\n",
       " ' mystery ': 16,\n",
       " ' news ': 17,\n",
       " ' reality-tv ': 18,\n",
       " ' romance ': 19,\n",
       " ' sci-fi ': 20,\n",
       " ' short ': 21,\n",
       " ' sport ': 22,\n",
       " ' talk-show ': 23,\n",
       " ' thriller ': 24,\n",
       " ' war ': 25,\n",
       " ' western ': 26}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train ,X_test,y_train,y_test=train_test_split(X[:20000],y[:20000],test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_strings = [' '.join(words) for words in X_train]\n",
    "X_test_strings = [' '.join(words)for words in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf=TfidfVectorizer(max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf.fit_transform(X_train_strings).toarray()\n",
    "X_test_tfidf = tfidf.transform(X_test_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.454"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "\n",
    "rf = RandomForestClassifier(max_depth=30)\n",
    "rf.fit(X_train_tfidf,y_train)\n",
    "\n",
    "y_pred=rf.predict(X_test_tfidf)\n",
    "\n",
    "accuracy_score(y_test,y_pred)\n",
    "# 0.43975\n",
    "# max depth = 30  -- 0.45\n",
    "# max depth = 30 with class_weight='balanced'  -- 0.315\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02025"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb=XGBClassifier(max_depth=20)\n",
    "xgb.fit(X_train_tfidf,y_train)\n",
    "\n",
    "y_pred1=xgb.predict(X_test_tfidf)\n",
    "\n",
    "accuracy_score(y_test,y_pred1)\n",
    "# 0.02025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2715"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb=GaussianNB()\n",
    "\n",
    "gnb.fit(X_train_tfidf,y_train)\n",
    "\n",
    "y_pred2=gnb.predict(X_test_tfidf.toarray())\n",
    "\n",
    "accuracy_score(y_test,y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4745"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "nb_classifier.fit(X_train_tfidf, y_train)\n",
    "y_pred3 = nb_classifier.predict(X_test_tfidf)\n",
    "\n",
    "accuracy_score(y_test,y_pred3)\n",
    "# 0.4745 in 0.3s\n",
    "# 0.51916 in 1.5s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8, 24,  1, ...,  7,  5, 12])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Batch Learning Using Multi-NB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5194444444444445\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[:54000], y[:54000], test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_strings = [' '.join(words) for words in X_train]\n",
    "X_test_strings = [' '.join (words) for words in X_test]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_strings).toarray()\n",
    "X_test_tfidf = tfidf.transform(X_test_strings).toarray()\n",
    "\n",
    "sgd_classifier = SGDClassifier(loss='log', random_state=42)\n",
    "\n",
    "batch_size = 5400\n",
    "\n",
    "\n",
    "# Iterate through mini-batches\n",
    "for i in range(0, len(X_train_strings), batch_size):\n",
    "    X_batch = X_train_strings[i:i + batch_size]\n",
    "    y_batch = y_train[i:i + batch_size]\n",
    "\n",
    "    # Vectorize the mini-batch using TfidfVectorizer\n",
    "    X_batch = tfidf.transform(X_batch).toarray()\n",
    "\n",
    "    # Train the model on the mini-batch\n",
    "    sgd_classifier.partial_fit(X_batch, y_batch ,classes=np.unique(y))\n",
    "\n",
    "y_pred4 = sgd_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred4)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Accuracy: 0.49525\n",
    "\n",
    "# with SGDClassifier\n",
    "# modified_huber\n",
    "# Accuracy: 0.5039814814814815\n",
    "# with loss = hinge\n",
    "# Accuracy: 0.5193518518518518"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aksha\\AppData\\Local\\Temp\\ipykernel_20116\\828271147.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  testdata= pd.read_csv(path, sep=':::', names=['Title','Genre', 'Description'])\n"
     ]
    }
   ],
   "source": [
    "path = \"./Genre Classification Dataset/test_data_solution.txt\"\n",
    "testdata= pd.read_csv(path, sep=':::', names=['Title','Genre', 'Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata['Description']=testdata['Description'].apply(lambda x: preprocess(x))\n",
    "testdata['Description']=testdata['Description'].apply(lambda x: remove_stopwords(x))\n",
    "testdata['Description']=testdata['Description'].apply(lambda x: word_tokenize(x))\n",
    "testdata['Description']=testdata['Description'].apply(lambda x: lemmatization(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata_strings = [' '.join(words) for words in testdata['Description']]\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "testdata_tfidf = tfidf.fit_transform(testdata_strings).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=[]\n",
    "pred = sgd_classifier.predict(testdata_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata['Predictions']=pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata['Predictions']=testdata['Predictions'].apply(lambda x: next((label for label, num in label_mapping.items() if num == x), 'Unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.26001845018450187\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(testdata['Genre'], testdata['Predictions'])\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
